{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsri45/GreenlightHCP/blob/master/mlp_demo_neuromorphic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTp24vxxYW2d"
      },
      "source": [
        "# MLP Demo (Worked Example)\n",
        "\n",
        "We will:\n",
        "1. Create a simple dataset (two interleaving moons)\n",
        "2. Train a small MLP in PyTorch\n",
        "3. Evaluate accuracy\n",
        "4. Visualize the decision boundary\n",
        "\n",
        "---\n"
      ],
      "id": "JTp24vxxYW2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZUcIT_dYW2d"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "id": "VZUcIT_dYW2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN7Qw2HLYW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Generate a toy dataset (2D points + binary labels)\n",
        "X, y = make_moons(n_samples=2000, noise=0.25, random_state=42)\n",
        "\n",
        "# Standardize features (helps training)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = torch.utils.data.TensorDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))"
      ],
      "id": "wN7Qw2HLYW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmM2lpz2YW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP(in_dim=2, hidden_dim=64, out_dim=2).to(device)\n",
        "print(model)"
      ],
      "id": "nmM2lpz2YW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-F8ALmIYW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.numel()\n",
        "    return correct / total\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=60, lr=1e-2):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\"train_acc\": [], \"val_acc\": [], \"loss\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader.dataset)\n",
        "        tr_acc = accuracy(model, train_loader)\n",
        "        va_acc = accuracy(model, val_loader)\n",
        "\n",
        "        history[\"loss\"].append(avg_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:03d} | loss {avg_loss:.4f} | train_acc {tr_acc:.3f} | val_acc {va_acc:.3f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train(model, train_loader, val_loader, epochs=60, lr=1e-2)"
      ],
      "id": "e-F8ALmIYW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWqWcDitYW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(history[\"loss\"])\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_acc\"], label=\"train\")\n",
        "plt.plot(history[\"val_acc\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Final val accuracy:\", accuracy(model, val_loader))"
      ],
      "id": "IWqWcDitYW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aMebQFyYW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_decision_boundary(model, X, y, title=\"Decision boundary\"):\n",
        "    model.eval()\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300)\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_t = torch.tensor(grid, dtype=torch.float32).to(device)\n",
        "\n",
        "    logits = model(grid_t)\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    Z = preds.reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(model, X_val, y_val, title=\"MLP decision boundary (validation set)\")"
      ],
      "id": "2aMebQFyYW2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziS7VlAYYW2e"
      },
      "source": [
        "# MLP Demo (Fill-in-the-Blank)\n",
        "\n",
        "Complete the TODOs to train an MLP classifier on the same moons dataset.\n",
        "\n",
        "**Tasks**:\n",
        "1. Build the model\n",
        "2. Implement the forward pass\n",
        "3. Implement the training step\n",
        "4. Verify you reach good validation accuracy\n",
        "\n",
        "---\n"
      ],
      "id": "ziS7VlAYYW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slHOzKPrYW2e"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "id": "slHOzKPrYW2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-HqbtoYW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "X, y = make_moons(n_samples=2000, noise=0.25, random_state=123)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=123, stratify=y\n",
        ")\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = torch.utils.data.TensorDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))"
      ],
      "id": "wm-HqbtoYW2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZAiRxoYYW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        # TODO(1): Define 3 Linear layers and 2 ReLU activations.\n",
        "        # Hint: the architecture is: Linear -> ReLU -> Linear -> ReLU -> Linear\n",
        "        self.fc1 = None  # TODO: nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = None  # TODO: nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = None  # TODO: nn.Linear(hidden_dim, out_dim)\n",
        "        self.relu = None # TODO: nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO(2): Implement the forward pass with ReLU between layers.\n",
        "        # Return logits (no softmax needed; CrossEntropyLoss expects logits).\n",
        "        pass\n",
        "\n",
        "model = MLP(in_dim=2, hidden_dim=64, out_dim=2).to(device)\n",
        "print(model)"
      ],
      "id": "XZAiRxoYYW2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54JjAEO4YW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.numel()\n",
        "    return correct / total"
      ],
      "id": "54JjAEO4YW2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1SfBYrzYW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "history = {\"loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "epochs = 60\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # TODO(3): Standard training step:\n",
        "        # - zero gradients\n",
        "        # - forward pass to get logits\n",
        "        # - compute loss\n",
        "        # - backprop\n",
        "        # - optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        logits = ...          # TODO\n",
        "        loss = ...            # TODO\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader.dataset)\n",
        "    tr_acc = accuracy(model, train_loader)\n",
        "    va_acc = accuracy(model, val_loader)\n",
        "\n",
        "    history[\"loss\"].append(avg_loss)\n",
        "    history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_acc\"].append(va_acc)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | loss {avg_loss:.4f} | train_acc {tr_acc:.3f} | val_acc {va_acc:.3f}\")"
      ],
      "id": "E1SfBYrzYW2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk-80eYzYW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(history[\"loss\"])\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_acc\"], label=\"train\")\n",
        "plt.plot(history[\"val_acc\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "final_val = accuracy(model, val_loader)\n",
        "print(\"Final validation accuracy:\", final_val)\n",
        "\n",
        "# Simple target threshold to confirm it worked (tweak if needed)\n",
        "assert final_val > 0.85, \"Try again: you should be able to reach > 0.85 val accuracy.\"\n",
        "print(\"Looks good!\")"
      ],
      "id": "qk-80eYzYW2f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FavLHJ3YW2f"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_decision_boundary(model, X, y, title=\"Decision boundary\"):\n",
        "    model.eval()\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300)\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_t = torch.tensor(grid, dtype=torch.float32).to(device)\n",
        "\n",
        "    logits = model(grid_t)\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    Z = preds.reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(model, X_val, y_val, title=\"Your MLP decision boundary (validation set)\")"
      ],
      "id": "3FavLHJ3YW2f"
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Start"
      ],
      "metadata": {
        "id": "ALe1u0_fp3Nm"
      },
      "id": "ALe1u0_fp3Nm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ML SECTION (Classical Baselines)\n",
        "# =========================\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "\n",
        "# Assumes these already exist from your preprocessing section:\n",
        "# X_train_s, X_val_s, X_test_s, y_train, y_val, y_test\n",
        "\n",
        "ml_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=3000, random_state=SEED),\n",
        "    \"SVM (RBF)\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=SEED),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=300, max_depth=None, min_samples_split=2, random_state=SEED\n",
        "    ),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in ml_models.items():\n",
        "    start = time.time()\n",
        "    model.fit(X_train_s, y_train)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    val_pred = model.predict(X_val_s)\n",
        "    test_pred = model.predict(X_test_s)\n",
        "\n",
        "    val_metrics = evaluate_predictions(y_val, val_pred)\n",
        "    test_metrics = evaluate_predictions(y_test, test_pred)\n",
        "\n",
        "    row = {\n",
        "        \"model\": name,\n",
        "        \"train_time_sec\": round(train_time, 3),\n",
        "        \"val_accuracy\": round(val_metrics[\"accuracy\"], 4),\n",
        "        \"val_f1\": round(val_metrics[\"f1\"], 4),\n",
        "        \"test_accuracy\": round(test_metrics[\"accuracy\"], 4),\n",
        "        \"test_precision\": round(test_metrics[\"precision\"], 4),\n",
        "        \"test_recall\": round(test_metrics[\"recall\"], 4),\n",
        "        \"test_f1\": round(test_metrics[\"f1\"], 4),\n",
        "    }\n",
        "    results.append(row)\n",
        "\n",
        "ml_results_df = pd.DataFrame(results).sort_values(by=\"test_f1\", ascending=False).reset_index(drop=True)\n",
        "ml_results_df\n"
      ],
      "metadata": {
        "id": "gRTxoaAnp5EG"
      },
      "id": "gRTxoaAnp5EG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_metrics_mlp should come from your DL section\n",
        "\n",
        "best_ml_f1 = ml_results_df.loc[0, \"test_f1\"]\n",
        "best_ml_acc = ml_results_df.loc[0, \"test_accuracy\"]\n",
        "best_ml_name = ml_results_df.loc[0, \"model\"]\n",
        "\n",
        "comparison_ml_vs_dl = pd.DataFrame([\n",
        "    {\"model\": best_ml_name, \"test_accuracy\": best_ml_acc, \"test_f1\": best_ml_f1},\n",
        "    {\"model\": \"MLP (DL)\", \"test_accuracy\": round(test_metrics_mlp[\"accuracy\"], 4), \"test_f1\": round(test_metrics_mlp[\"f1\"], 4)},\n",
        "])\n",
        "\n",
        "comparison_ml_vs_dl\n"
      ],
      "metadata": {
        "id": "HaMnVToWp51N"
      },
      "id": "HaMnVToWp51N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}